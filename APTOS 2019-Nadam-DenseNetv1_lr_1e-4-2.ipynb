{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "\n",
    "import os\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "print(os.listdir(\"../input\")) # note: the densenet-keras is added manually to implement transfer learning from densenet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import cv2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import (Activation, Dropout, Dense,\n",
    "                          BatchNormalization, Input, GlobalAveragePooling2D)\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras.optimizers import Nadam\n",
    "#from keras import regularizers, optimizers #doubtfull\n",
    "\n",
    "from tqdm import tqdm #shows progress bar of the operation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "####to be deleted####\n",
    "# from keras.layers import Conv2D\n",
    "# from keras.layers import MaxPooling2D\n",
    "# from keras.layers import Flatten\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import GaussianDropout\n",
    "# from keras import regularizers, optimizers\n",
    "# from keras.regularizers import l1,l2\n",
    "\n",
    "\n",
    "#  #this is new for me\n",
    "# from PIL import Image #this is new for me\n",
    "\n",
    "\n",
    "# import scipy #this is new for me\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from keras.models import Sequential, load_model\n",
    "\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# from keras import metrics\n",
    "# from keras.optimizers import Adam \n",
    "# from keras.optimizers import Nadam \n",
    "\n",
    "# from keras import backend as K\n",
    "# import keras\n",
    "# from keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "#print(\"tensorflow_version=\",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed for repeatability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "tf.set_random_seed(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "test_df  = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"test_df shape:\",test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at data distribution\n",
    "train_df['diagnosis'].hist()\n",
    "train_df['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samples(df, columns=4, rows=3):\n",
    "    fig=plt.figure(figsize=(5*columns, 4*rows))\n",
    "\n",
    "    for i in range(columns*rows):\n",
    "        image_path = df.loc[i,'id_code']\n",
    "        image_id = df.loc[i,'diagnosis']\n",
    "        img = cv2.imread(f'../input/aptos2019-blindness-detection/train_images/{image_path}.png')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        print('size = ',img.shape)\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.title(image_id)\n",
    "        plt.imshow(img)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "display_samples(train_df,2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize Images\n",
    "Resize to 312x312, then create a single numpy array to hold the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = train_df.shape[0]\n",
    "IMG_SIZE = 312\n",
    "x_train = np.zeros((N, IMG_SIZE, IMG_SIZE,3), dtype = np.uint8)\n",
    "\n",
    "def resize_IMG(image_path, desired_size = IMG_SIZE):\n",
    "    img = cv2.imread(image_path)\n",
    "    im = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    im = cv2.resize(im,(desired_size, desired_size))\n",
    "    \n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resize the train set images and store in x_train array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image_id in enumerate(tqdm(train_df['id_code'])):\n",
    "    x_train[i, :, :, :] = resize_IMG(\n",
    "    f'../input/aptos2019-blindness-detection/train_images/{image_id}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the y values to categorical values (0,1,2,3,4)\n",
    "y_train = tf.keras.utils.to_categorical(\n",
    "    train_df['diagnosis'],\n",
    "    num_classes=5,\n",
    "    dtype='uint8'\n",
    ")\n",
    "y_train[0:5] #see the first 5 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train_test split\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(x_train, y_train, test_size=0.20,\n",
    "                                                      stratify=y_train, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting dataset to generate more training samples to address class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Augmenting the training set to get more training set out of it as wells as help reduce overfitting\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "datagen = ImageDataGenerator(\n",
    "    zoom_range = 0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=(-3.0,3.0),\n",
    "    channel_shift_range=5.0\n",
    "    )\n",
    "\n",
    "data_generator = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE, seed=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional - only for sanity check that preprocessing is done ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets visualize the train and test images after preprocessing and split\n",
    "def display_samples_preprocess(df1,df2, columns=4, rows=3):\n",
    "    fig=plt.figure(figsize=(5*columns, 4*rows))\n",
    "\n",
    "    for i in range(columns*rows):\n",
    "        train_img = df1[i] \n",
    "        #print('label:',train_y[i])\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.title('train_x{}, train_y:{}'.format(i,df2[i]))\n",
    "       \n",
    "        plt.imshow(train_img)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples_preprocess(train_x,train_y,2,2)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure validation sets also looks ok\n",
    "display_samples_preprocess(valid_x,valid_y,5,2)\n",
    "valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Callback class to implement \n",
    "class Metrics(Callback):\n",
    "    \"\"\" Callback class to be used to calculate validation kappa score after each epoch and save model if higher \n",
    "    value than last saved value\"\"\"\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_kappas = []\n",
    "    \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        X_val, y_val = self.validation_data[:2]\n",
    "        y_pred = self.model.predict(X_val)\n",
    "       \n",
    "        def flatten(var):\n",
    "            flat = []\n",
    "            for row in tqdm(var):\n",
    "                predict_label = np.argmax(row)\n",
    "                flat.append(str(predict_label))\n",
    "            return flat\n",
    "        \n",
    "        y_val = flatten(y_val)\n",
    "        y_pred = flatten(y_pred)\n",
    "\n",
    "        _val_kappa = cohen_kappa_score(\n",
    "            y_val,\n",
    "            y_pred, \n",
    "            weights='quadratic'\n",
    "        )\n",
    "\n",
    "        self.val_kappas.append(_val_kappa)\n",
    "\n",
    "        print(f\"val_kappa: {_val_kappa:.4f}\")\n",
    "        \n",
    "        if _val_kappa >= max(self.val_kappas):\n",
    "            print(\"Validation Kappa has improved. Saving model.\")\n",
    "            self.model.save('../working/model.h5')\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the two callback functions that will be called during training\n",
    "early =  EarlyStopping(monitor = \"val_accuracy\",\n",
    "                      mode = \"max\",\n",
    "                      patience = 20)\n",
    "\n",
    "kappa_metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model:\n",
    "\n",
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape = input_shape)\n",
    "    base_model = DenseNet121(include_top = False,\n",
    "                            weights = None,\n",
    "                            input_tensor = input_tensor)\n",
    "    base_model.load_weights(\"../input/densenet-keras/DenseNet-BC-121-32-no-top.h5\")\n",
    "    x = GlobalAveragePooling2D()(base_model.output) # learn about it\n",
    "    x = Dense(1024, activation = 'relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    final_output = Dense(n_out, activation = 'softmax', name = 'final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model:\n",
    "\n",
    "NUM_CLASS = 5\n",
    "\n",
    "model = create_model(\n",
    "        input_shape= (IMG_SIZE, IMG_SIZE, 3),\n",
    "        n_out = NUM_CLASS\n",
    "                    )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm up training\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for i in range(-2,0):\n",
    "    model.layers[i].trainable = True\n",
    "\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = Nadam(0.0001),\n",
    "    metrics =['accuracy']\n",
    ")\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit_generator(data_generator,\n",
    "                              steps_per_epoch=x_train.shape[0]/BATCH_SIZE,\n",
    "                              epochs = 2, verbose =1,validation_data = (valid_x, valid_y),\n",
    "                              validation_steps = np.int(valid_x.shape[0] / BATCH_SIZE),\n",
    "                              callbacks = [kappa_metrics, early]\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  \"\"\" to plot training and validation accuracy trend per epoch\"\"\"  \n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Accuracy')\n",
    "  \n",
    "  plt.plot(hist['epoch'], hist['accuracy'],\n",
    "          label ='Train accuracy')\n",
    "  \n",
    "  plt.plot(hist['epoch'], hist['val_accuracy'],\n",
    "           label = 'Val accuracy')\n",
    "  \n",
    "  plt.legend()\n",
    "  #plt.ylim([0,5])\n",
    "  \n",
    "  \n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  \n",
    "  plt.plot(hist['epoch'], hist['loss'],\n",
    "          label = 'Train loss')\n",
    "  plt.plot(hist['epoch'], hist['val_loss'],\n",
    "          label = 'Val loss')\n",
    "  #plt.ylim[0,5]\n",
    "  \n",
    "  plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train all layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "   \n",
    "model.compile(\n",
    "loss = 'categorical_crossentropy',\n",
    "optimizer = Nadam(0.0001),\n",
    "\n",
    "metrics =['accuracy'])\n",
    "\n",
    "history = model.fit_generator(data_generator,\n",
    "                              steps_per_epoch=x_train.shape[0]/BATCH_SIZE,\n",
    "                              epochs = 50, verbose =1, \n",
    "                              validation_data = (valid_x, valid_y),\n",
    "                              validation_steps = np.int(valid_x.shape[0] / BATCH_SIZE),\n",
    "                              callbacks = [kappa_metrics, early]\n",
    "                               )\n",
    "\n",
    "print('plotting history for lr=',0.0001)\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last saved model to get the best model weights\n",
    "model.load_weights('../working/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a sense of model performance using validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_predict = model.predict(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten y values to get only the diagnostic Id\n",
    "def flatten(var):\n",
    "    flat = []\n",
    "    for row in tqdm(var):\n",
    "        predict_label = np.argmax(row)\n",
    "    \n",
    "    #print(predict_label)\n",
    "        flat.append(str(predict_label))\n",
    "    return flat\n",
    "\n",
    "validation = flatten(score_predict)\n",
    "print(validation[0:5])\n",
    "\n",
    "valid_y_flat = flatten(valid_y)\n",
    "\n",
    "print(valid_y_flat[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation[20:50])\n",
    "print(valid_y_flat[20:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_kappa(true, pred):\n",
    "        val_kappa = cohen_kappa_score(\n",
    "            true,\n",
    "            pred, \n",
    "            weights='quadratic'\n",
    "        )\n",
    "        return val_kappa\n",
    "#\n",
    "score = cohen_kappa(valid_y_flat, validation)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same preprocessing for test_set\n",
    "N1 = test_df.shape[0]\n",
    "x_test = np.zeros((N1, IMG_SIZE, IMG_SIZE, 3), dtype = np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make predictions on the test set \n",
    "predicted = []\n",
    "model.load_weights('../working/model.h5')\n",
    "\n",
    "for i, image_id in enumerate(tqdm(test_df['id_code'])):\n",
    "    x_test[i, :, :, :] = resize_IMG(\n",
    "        f'../input/aptos2019-blindness-detection/test_images/{image_id}.png')\n",
    "    predicted_temp = model.predict(x_test[i].reshape(1,IMG_SIZE,IMG_SIZE,3))\n",
    "    #print(predicted_temp)\n",
    "    predicted.append(predicted_temp)\n",
    "    \n",
    "# note: test set is inferenced by each example, not on entire set (storing the full test set after resizing \n",
    "# causes memory overload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the predicted to get the predicted_lable as a column vector\n",
    "predicted_label = flatten(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the submission file\n",
    "submit = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "submit['diagnosis'] = predicted_label\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['diagnosis'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
